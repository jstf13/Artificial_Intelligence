Clase 16/05/23 - Grupo N7A - Min 1:47

MOUNTAIN CAR
- En el MountainCar el archivo MountainCarEnv.py es el enviroment que no lo tenemos ni
  que tocar.
- El agent.ipynp es para interactuar con el entorno.
- El mountaincar.ipynp se podría usar como no, es opcional.
- En la última parte se simula lo que tendríamos que hacer nosotros en Q-Learning. Elijo
  la acción a realizar con el epsilon_greedy_policy que será 0, 1 ó 2 si corresponde
  hacer exploración ó el máx Q si corresponde hacer exploit.
- El epsilon_greedy_policy pasa un epsilon de 0.1, luego asigna a explore el valor de 
  binomial(1, epsilon) que es la probabilidad real. El epsilon, para priorizar la 
  exploración debería ser cercano a 1 y luego se irá achicando para que explote.
- Es importante estudiar y variar los valores de:
    pos_space = np.linspace(-5, 5, 10)
    vel_space = np.linspace(-3, 3, 2)

  Hay que variar los parámetros, estados y los distintos valores.

CONNECT FOUR
- Es el juego 4 en línea.
- Se nos da una notebook Connect_four.ipynp que es un ambiente hecho de cero.
- Jugamos nosotros contra la IA.
- La IA estará jugando como lo indique el notebook spaceGPT_agent_obf.py.
- Está el enviroment connect_four_env.py y el board hecho en board.py.
- También es agente es agent.py.
- Al ejecutar el Connect_four.ipynp se debe ingresar un número del 0 al 6 con
  que se desea jugar cada vez.

Clase 18/05/23 - Grupo M7B

- MountainCar es con Q-Learning.
- El sistema antiaburrimiento será utilizando minimax y expectimax.
- La idea es que la notebook que usemos para ejecutar tenga el código mínimo
  para ejecutar. Luego debemos desarrollar un código en python.
- Es recomendable que no tengamos notebooks complejas para evitar errores.
- Una vez tengamos una notebook armada, que digamos que es la que queremos 
  ejecutar, ahí generamos un HTML o PDF con la ejecución de la notebook.
- También un breve informe en formato PDF.
- Todo debe ir guardado en un zip que se deberá subir a gestión.
- Luego nos darán los lenamientos del informe.
- Lo que debemos hacer es la experimentación de mucho código por todos lados,
  incluso podemos consultar a ChatGPT para que nos genere código.
- La dificultad estará en organizar bien los expermientos, poder compararlos y 
  variar los parámetros.
- En Q-Learning variar el gamma, el alfa y el epsilon. Debemos discretizar los 
  estados.
- Debemos comparar, presentar y recurrir a técnicas y herramientas de visualización.
  Por ejemplo plotear una curva de los valores que vamos observando.
- Se sugiere que generemos logs.
- Es importante que si usamos por ejemplo ChatGPT, el libro o cualquier otra fuente
  para el código lo nombremos. Es muy importante la parte experimental.
- Debemos incluir un resumen de como abordar la tarea, incluyendo información relevante,
  como atacar un cierto problema, como abordamos por ejemplo el análisis de los 
  parámetros, si todo junto, si fuimos variando de a uno, etc. Cómo descompusimos el 
  problema, cuál atacamos, si atacamos todo junto, la discretización de los parámetros, 
  etc. Cómo medimos y agregar un apoyo visual mediante herramientas de python que nos 
  permitan plotear curvas.
  A veces es mejor poner tablas y otras es mejor curvas. En particular, en Q-Learning
  los algoritmos son iterativos y pueden ir evolucionando. Esta evolución tiene 
  sentido mostrarlo como curvas para ver cómo fue mejorando o empeorando por ejemplo Q.
  Cómo va evolucionando la estrategia. 
  Cualquier nota de advertencia para comunicar a la empresa.
- Es importante que sea la versión 0.26 de gym.
- LAS EJECUCIONES VAN A DEMORAR.

MOUNTAIN CAR
- El MountainCarEnv.py es el ambiente que está adaptado a Gym.
- El agent.ipynp es la notebook para probar el entorno y la simulación del ambiente.
- El mountaincar.ipynp es una notebook con cosas que nos pueden servir.
- Para el código, el algoritmo y el tratamiento de los parámetros, lo mejor es crear
  un archivo python y después una notebook para ver los resultados y comparar.
- El ActionSpace son las 3 acciones que son 0,1 y 2. Donde 0 es acelerar a la izquierda,
  1 es no acelerar y 2 es acelerar a la derecha.
- Las transiciones están basadas en el ObservationSpace que es 0, la posición del carrito
  y 1 que es la velocidad. Tenemos que discretizar la posición contínua al igual que la 
  velocidad para poder trabajar.
- En Transition Dynamics se dan las leyes de la física que actualizan los valores.
- La recompensa es por cada movimiento que hace y es -1.
- La cantidad de episodios máxima que se puede hacer en una corrida está topeada en
  500. Lo peor que puede pasar es que la recompensa sea de -500.
- Si la recompensa da más de -500, entonces pudo llegar antes de los 500 episodios
  a la cima.
- Si no llega el carrito a la bandeja, la recompensa será de -500. Serían 500 pasos.
- Siempre trabajaremos con recompensas negativas.
- La posición inicial es un valor randómico entre -0,6 y -0,4 y la velocidad inicial
  es siempre 0.
- El episodio puede terminar si llegó a la cima o si el largo del episodio es 500.
- Hay 2 render_mode, uno es human que nos mostrará una interfaz gráfica con el carrito
  moviéndose e intentando llegar a la cima y el otro es rgb_array que no lo vamos a usar
  pero cuando realicemos el algoritmo hay que pasarle el rgb_array para que no muestre
  todo el tiempo la interfaz gráfica mientras está aprendiendo.
- Lo primero que deberíamos hacer es jugar con el ambiente, probar cuales son las 
  reacciones, probar las acciones, cuáles son las operaciones iniciales, hago reset, etc.
- La notebook mountaincar.ipynp tiene cosas que podemos utilizar si así lo preferimos.
- Tenemos:
    pos_space = np.linspace(-5, 5, 10)
    vel_space = np.linspace(-3, 3, 2)
  Vamos a tener que discretizar las observaciones de posición (pos_space) y velocidad
  (vel_space) para no tener infinitos estados.
  Queremos tener una cantidad de estados controlada.
- np.linspace(-5, 5, 10) nos da 10 divisiones entre -5 y 5.
- np.linspace(-3, 3, 2) nos da 2 divisiones entre -3 y 3, así tendríamos 20 estados
  (10x2)
- El def get_state(obs) recibe una observación del ambiente y nos diría por ejemplo
  a qué posición correspondería para poder actualizar en el Q esa observación y una 
  determinada acción.
- El numpy.digitize retorna el índice de lo que se llama bins, que sería lo anterior, que
  sería el valor que le corresponde en el array.
- En state = get_state(np.array([-0.4, 0.2])), el np.array sería la observación y nos 
  devuelve el estado real.
  Que es el estado que manejaremos en el ambiente.
- En Q = np.zeros((11, 3, 3)) inicializamos el Q. Ahí son ceros. El 11 es porque son 10
  divisiones, luego el primer 3 es porque son 2 divisiones y el último 3 es porque son 
  3 acciones.
- El Q puede ser un diccionario o un np.array. El np.array es mucho más rápido.
  Cargo el Q de el estado y la acción en el array tridimensional.
- Lo último del mountaincar.ipynp sería el Q-Learning incompleto. Por ejemplo, falta el
  alfa, el gamma y demás.
- En epsilon_greedy_policy, al principio lo que más conviene es explorar dejando el 
  epsilon en un valor alto. Y al final en un valor bajo para poder explotar. Esto se
  espera que se haga el análisis, de cómo se varió el epsilon y qué resultados dio.
- Se recomienda crear un archivo en python, armar el algoritmo, usar el 
  epsilon_greedy_policy y los tratamientos de los parámetros, ver la cantidad de estados
  generados, lo cual depende de la cantidad de estados que se hagan en el np.linspace. Si
  se hacen muchos estados, seguramente van a generar más episodios y si se hacen pocos, 
  se generarán menos.
- En Q-Learning hay dos visualizaciones interesantes, una es la fase de aprendizaje para
  aprender la policy óptima pero no la ejecuta.

PROCEDIMIENTO:
discretización
loop:
    - aprendizaje: Q-Learning {
                    - Cantidad de iteraciones (Se necesitan variar)
                    - Gamma
                    - Alfa
                    - Epsilon
                } ====> Esto genera la salida Q, π*
    - ejecución: Cantidad de iteraciones (medir recompensas acumuladas)
      (control)
      (sería como el test de Machine Learning)
      (Se pueden registrar los valores de las recompensas obtenidas en cada 
      episodio ejecutado de acuerdo a la policy y el promedio ó total de 
      recompensas sobre todos los episodios)


- Nos puede servir el Pickle, de extensión .pkl nos sirve para guardar por 
  ejemplo un Q que entrenamos previamente por horas. Se puede usar sólo si se 
  mantiene la discretización, si se cambia ya no sirve.
- Podemos usar Weights & Biases (https://wandb.ai/site) para el documento,
  podemos hacer captura del log que se guarda con Weights & Biases.

Clase 23/05/23 - Grupo N7A

MOUNTAIN CAR
- Para el obligatorio, el Q nos dirá cuánto es el Q para un estado y una acción
  y nos dará la próxima acción que depende de si estoy explorando ó explotando lo
  que yo ya tengo.
- La idea es que podamos implementar la actualización de Q en base a la matríz, 
  como ir actualizando ese Q y el alfa.

------------------------------------------------------------------------------------------------------------------
CONSULTA FEDERICO ARMANDO 30/06/23
- En el ConnnectFour lo que se pide es implementar un agente que pueda jugar con Oponente que es el agente spaceGPT_agent_obf
- Debemos cambiar el InputAgent por uno que sea Minimax o Expectimax. Uno de los 2. Para ello, debemos definir una función
  de evaluación (heuristic_utility) que sea buena y que cumpla los criterios que vimos
- Si nos fijamos, el heuristic_utility ya recibe el board actual, donde debemos clonar y llamar recurrentemente para poder
  obtener el proximo estado que nos maximice ganar.
- Podemos tomar como base el InputAgent y complementar los métodos que faltan
------------------------------------------------------------------------------------------------------------------
FUNCIONES DE EVALUACIÓN SUGERIDAS POR ChatGPT PARA ConnectFour

Aquí tienes algunas ideas para funciones de evaluación heurística que podrías utilizar en el juego Connect Four:

1. Conteo de fichas en línea: Esta función evalúa la cantidad de fichas del jugador en línea en todas las direcciones posibles (horizontal, vertical, diagonal). Puedes asignar un puntaje más alto a las líneas con más fichas del jugador y un puntaje más bajo a las líneas con fichas del oponente.

2. Espacios adyacentes: Evalúa la cantidad de espacios adyacentes vacíos a las fichas del jugador. Cuantos más espacios vacíos adyacentes haya, mayor será el puntaje. Esto podría indicar una mayor oportunidad de formar una línea ganadora.

3. Bloqueo de oponente: Evalúa la cantidad de líneas en las que el oponente tiene más fichas consecutivas y menos espacios vacíos. Puedes asignar un puntaje negativo para indicar la necesidad de bloquear esas líneas.

4. Control del centro del tablero: Evalúa si el jugador tiene fichas en el centro del tablero. El centro del tablero proporciona más oportunidades de formar líneas en diferentes direcciones. Puedes asignar un puntaje más alto si el jugador tiene fichas en el centro.

5. Combinación de estrategias: Puedes combinar varias estrategias anteriores y asignar diferentes pesos a cada una de ellas para obtener una función de evaluación más completa. Por ejemplo, puedes sumar los puntajes de conteo de fichas en línea, espacios adyacentes y control del centro del tablero.

Recuerda que la elección de la función de evaluación depende de tus objetivos específicos y del estilo de juego que deseas adoptar. Puedes experimentar con diferentes enfoques y ajustar los pesos y puntajes según sea necesario para obtener un agente que juegue de manera efectiva en el juego Connect Four.

Espero que estas ideas te sean útiles en la creación de funciones de evaluación heurística para tu agente Minimax en el juego Connect Four.

